---
title: "Introduction to Probabilistic Machine Learning with Stan"
author: "Daniel Emaasit"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_notebook:
    highlight: textmate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, tidy = FALSE, cache = TRUE, comment = NA,
               fig.width = 7, fig.height = 5, warning = FALSE, echo = TRUE, eval = FALSE)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(plotly))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(MASS))
theme_set(theme_bw)
```

# Introduction
Let's load the required libraries.
```{r eval = FALSE}
library(rstan)
library(tidyverse)
library(bayesplot)
library(MASS)
library(gridExtra)
library(hrbrthemes)
rstan_options(auto_write = TRUE) #To write the stan object to the hard disk using saveRDS
options(mc.cores = parallel::detectCores())
```

# Parametric Bayesian Methods

## Data
https://archive.ics.uci.edu/ml/datasets/Auto+MPG
```{r}
mpg_data <- read_csv("data/auto_mpg_data.csv") %>% na.omit()
head(mpg_data)
```

# Nonparametric Bayesian Methods

## Gaussian Process Joint Hyperparameter Fitting and Predictive Inference

### Step 1: Prepare the input data
```{r}
library(tidyverse)

mpg_data <- read_csv("data/auto_mpg_data.csv") %>% na.omit()
mpg_data$id <- 1:nrow(mpg_data)
train <- sample_frac(mpg_data, 0.7)
test  <- anti_join(mpg_data, train, by = 'id')

y <- train$mpg
x <- as.matrix(train[, 3])
N <- nrow(x)
D <- ncol(x)
x_pred <- as.matrix(test[, 3])
N_pred <- nrow(x_pred)

length_scale_shape <- 150
length_scale_rate <- 1
signal_variance_scale <- 1
noise_variance_scale <- 0.1

my_data_current <- list(y = y, x = x, N = N, D = D, x_pred = x_pred, N_pred = N_pred, 
                        length_scale_shape = length_scale_shape, 
                        length_scale_rate = length_scale_rate, 
                        signal_variance_scale = signal_variance_scale, 
                        noise_variance_scale = noise_variance_scale)
```

### Step 2: Build the model
The model is prepared as a "Stan program" in a separate stan file named "gp_joint_current.stan". The specified Stan program encodes a joint hyperparameter fit and predictive inference model, by declaring the hyperparameters as additional parameters and giving them priors.

```{r}
gp_joint_current <- "
data {
  int<lower=1> N;
  int<lower=1> D;
  int<lower=1> N_pred;
  vector[N] y;
  vector[D] x[N];
  vector[D] x_pred[N_pred];
}
parameters {
  real<lower=1e-12> length_scale;
  real<lower=0> alpha;
  real<lower=1e-12> sigma;
  vector[N] eta; 
}
transformed parameters {
  vector[N] f;
  {
     matrix[N, N] L_cov;
     matrix[N, N] cov;
     cov = cov_exp_quad(x, alpha, length_scale);
     for (n in 1:N)
       cov[n, n] = cov[n, n] + 1e-12;
     L_cov = cholesky_decompose(cov);
     f = L_cov * eta; 
  }
}
model {
  length_scale ~ student_t(4,0,1); # (df, mean, sd)
  alpha ~ normal(0, 1);
  sigma ~ normal(0, 1);
  eta ~ normal(0, 1);
  y ~ normal(f, sigma);
}
"
```

### Step 3: Sample from the posterior
```{r}
my_fit_joint <- stan(model_code = gp_joint, data = my_data, iter = 200, chains = 3)
```


### Step 4: Diagnose the results
```{r}
list_of_draws <- rstan::extract(my_fit_joint)
print(names(list_of_draws))
print(my_fit_joint)
```

#### Predictive Accuracy
```{r}
# find_mode <- function(x) {
#   ux <- unique(x)
#   ux[which.max(tabulate(match(x, ux)))]
# }
# y_pred_draws <- list_of_draws$y2 %>% as.data.frame()
# y_pred <- lapply(y_pred_draws, find_mode) %>% as.data.frame()
y_pred <- colMeans(list_of_draws$y2) %>% as.data.frame()
y_actual <- test$mpg 
y_combined <- data.frame(y_pred = y_pred, y_actual = y_actual)

# Function that returns Root Mean Squared Error
rmse <- function(y_actual, y_pred) {
  error <- y_actual - y_pred
  sqrt(mean(error^2))
}

# Function that returns Mean Absolute Error
mae <- function(y_actual, y_pred) {
  error_abs <- abs(y_actual - y_pred)
  colnames(error_abs) <- "absolute_error"
  mean(error_abs$absolute_error)
}

rmse(y_actual, y_pred)
mae(y_actual, y_pred)
```


# Appendix

## Compare with linear model
```{r}
library(caret)
##fit a linear model using "lm" method from caret package
lm_fit<-train(mpg ~ disp + hp + drat + wt + qsec, method="lm", data = train)
##then use the model to predict new values
lm_predict<-predict(lm_fit, newdata = test)
rmse(y_actual, y_pred = as.data.frame(lm_predict))
mae(y_actual, y_pred = as.data.frame(lm_predict))
```
```{r}
postResample(pred = lm_predict, obs = test$mpg) ## my rmse is the same as from caret
```


```{r}
library(ggplot2)
ggplot(data = head(mpg_data, n = 10), aes(x = disp, y = mpg)) + 
  geom_point() + 
  geom_smooth(aes(color = "Polynomial"), method = "lm", 
              formula = (formula= (y ~ poly(x, 4))))
```