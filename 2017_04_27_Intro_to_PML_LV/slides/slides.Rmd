---
title: '<h2>Introduction to Probabilistic Machine Learning with Stan</h2>'
subtitle: " <br/><h3>[R & Data Science Meetup - Las Vegas]()</h3>"
author: ' <br/><h3>[Daniel Emaasit](http://www.danielemaasit.com/)<br/>PhD Student<br/>Department of Civil Engineering<br/>University of Nevada, Las Vegas<br/>[daniel.emaasit@gmail.com](mailto:daniel.emaasit@gmail.com)</h3>'
date: "<h4>[`r format(Sys.time(), '%B %d %Y')`]()</h4>"
output:
  revealjs::revealjs_presentation:
    logo: images/unlv-logo.png
    fig_caption: no
    fig_height: 6
    fig_width: 7
    incremental: false
    css: slides.css
    theme: night
    highlight: espresso
    center: true
    transition: none
    background_transition: default
    self_contained: false
    reveal_plugins: ["notes", "zoom", "chalkboard"]
    reveal_options:
      slideNumber: true
      previewLinks: true
      chalkboard:
        theme: whiteboard
        toggleNotesButton: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

## Materials
Download slides & code: https://tinyurl.com/pml-lv

## Application 1
* Probabilistic approach to ranking & matching gamers.

```{r echo = FALSE, out.width = "80%", fig.cap= "Herbrich et al., 2009. (Microsoft Research)"}
knitr::include_graphics("images/xbox.jpg")
```

## Application 2
* Optimizing expensive functions in autonomous vehicles (using Bayesian optimization).

```{r echo = FALSE, out.width = "80%", fig.cap= "Schneider et al., 2016. (Uber ATG)"}
knitr::include_graphics("images/uber.jpg")
```

## Application 3
* Supplying internet to remote areas (using Gaussian processes)

```{r echo = FALSE, out.width = "65%", fig.cap= "Wired magazine, accessed 2016. (Google Project Loon)"}
knitr::include_graphics("images/project-loon.png")
```

## Application 4
* Exploring millions of taxi trajectories (using Gaussian mixture models)

```{r echo = FALSE, out.width = "50%", fig.cap= "Kucukelbir et al., 2016. (Columbia University)"}
knitr::include_graphics("images/taxi-trajectories.png")
```

# Intro to Probabilistic Machine Learning  {data-background=#74d1ea}

## ML: A Probabilistic Perspective (1/2)
* Probabilisic ML:
      + a set of methods that can automatically learn models of data (Murphy, 2013; Ghahramani, 2014)  
<br> 

* A Model:
      + A model describes data that one could observe from a system (Ghahramani, 2014)  
<br> 

* How to deal with uncertainty in models?
      + Use the mathematics of probability theory to express all forms of uncertainty  
<br> 

## ML: A Probabilistic Perspective (2/2)
```{r echo = FALSE, out.width = "80%"}
knitr::include_graphics("images/bayes-theorem.png")
``` 
     
## Probabilisic ML Vs Traditional/Algorithmic ML

 #  |   Algorithmic ML                  | Probabilistic ML
---|    -------------                 |------------------
**Examples** |    K-Means, Random Forest    |  GMM, Gaussian Process 
**Specification** |    Model + Algorithm integrated    |  Model & Inference separate
**Unknowns** |    Parameters    |  Random variables
**Inference** |    Optimization (MLE)             |  Bayes (MCMC, VI)
**Regularization** |    Penalty terms | Priors 
**Solution** |    Best fitting parameter     |  Full posterior

## Probabilisic Programming
* Probabilisic Programming (PP) Languages:
      + Software packages that take a model and then automatically generate inference routines (even source code!) e.g Stan, Infer.Net, PyMC3, etc.   
<br>  

* Steps in PML:
      + Build the model (Joint probability distribution of all the relevant variables)
      + Incorporate the observed data
      + Perform inference (To learn parameters of the latent variables)  
<br>

```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics("images/pp.png")
```

<!-- ## PML Process -->

<!-- ```{r echo = FALSE, out.width = "80%", fig.cap= "Stan Group (2017)"} -->
<!-- knitr::include_graphics("images/pml-cycle.jpg") -->
<!-- ``` -->


# How does Stan work?  {data-background=#74d1ea}

## Bayesian <span style="color:red">Linear</span> Model (1/4)

* **Objective**: predict ***mpg*** as a function of ***displacement (disp)***.
\begin{aligned}
y &= f(\textbf{x}) + \varepsilon, \\
f(\textbf{x}) &= \beta_0 + \beta\textbf{x}.
\end{aligned}  

      + Assume additive noise  
      + Assume $\varepsilon \stackrel{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$   
<br>

```{r echo = FALSE}
library(dplyr)
library(DT)
mtcars$id <- 1:nrow(mtcars)
train <- dplyr::sample_frac(mtcars, 0.5)
test  <- dplyr::anti_join(mtcars, train, by = 'id')
datatable(mtcars, class = 'cell-border stripe', 
          options = list(pageLength = 5))
```

## Bayesian <span style="color:red">Linear</span> Model (2/4)
* **Step 1**: Define priors on the parameters, $\theta$
      + Intercept: $\beta_0 \sim \mathcal{N}(0, 20)$  
      + Slope: $\beta_1 \sim \mathcal{N}(0, 15)$
      + Noise-deviation: $\sigma \sim \mathcal{N}(0, 5)$ 
<div>
```{r fig.height = 5, fig.align='default'}
intercept_df <- data_frame(rnorm(n = 10000, mean = 0, sd = 20)) 
colnames(intercept_df) <- "intercept"
slope_df <- data_frame(rnorm(n = 10000, mean = 0, sd = 15)) 
colnames(slope_df) <- "slope"
noise_df <- data_frame(rnorm(n = 10000, mean = 0, sd = 10)) 
colnames(noise_df) <- "noise"
params_plot <- ggplot() + 
  geom_density(data = intercept_df, aes(x = intercept, fill = "intercept"), alpha = 0.2) + 
  geom_density(data = slope_df, aes(x = slope, fill = "slope"), alpha = 0.2) + 
  geom_density(data = noise_df, aes(x = noise, fill = "noise"), alpha = 0.2) + 
  theme_bw() + theme(legend.position = "bottom") +
  xlab('parameters') + 
  scale_fill_manual(name = '', values = c('intercept'='green',
                                           'slope'= 'blue',
                                           'noise'='red'))
ggplotly(params_plot)
```
</div>

## Bayesian <span style="color:red">Linear</span> Model (3/4)
* **Step 2**: Define the likelihood
$$
\begin{aligned}
P(\textbf{y}\mid\textbf{x}, \mathbf{\theta}) & = \mathcal{N}(\mu, \sigma^{2}) = \mathcal{N}(\beta_0 + \beta\textbf{x}, \sigma^{2})
\end{aligned}
$$

* **Step 3**: Find the posterior of parameters
$$
\begin{aligned}
P(\theta \mid \textbf{y}, \textbf{x}) &\propto P(\theta ) P(\textbf{y} \mid \textbf{x}, \mathbf{\theta})
\end{aligned}
$$    

```{r fig.height = 4}
library("bayesplot")
color_scheme_set("brightblue")
my_fit_lm <-readRDS("my_fit_lm_mtcars.RDS")
names(my_fit_lm)[2] <- "slope"
names(my_fit_lm)[3] <- "noise_variance"
array_of_draws <- as.array(my_fit_lm)
mcmc_dens(array_of_draws, 
          pars = c("intercept", "slope", "noise_variance"), 
          facet_args = list(labeller = ggplot2::label_parsed)) + 
  facet_text(size = 13, color = "darkblue") +
  ggtitle("Posterior distributions of parameters", 
          subtitle = "from Bayesian Linear Regression")
```
      
## Bayesian <span style="color:red">Linear</span> Model (4/4)
* **Step 4**: Validate the model on test data, $\textbf{x}_*$

$$
\begin{aligned}
P(\textbf{y}_* \mid \textbf{x}_*, \textbf{x, y}) = \int P(\theta \mid \textbf{y}, \textbf{x}) P(\textbf{y}_*\mid\textbf{x}_*, \theta) \, d\theta
\end{aligned}
$$  
```{r fig.height=5}
p1 <- ggplot(test, aes(x = test$disp, y = test$mpg)) +
  geom_point(aes(colour = 'Test data'), size = 3) +
  geom_abline(aes(intercept = 28.31, slope = -0.04, colour = 'Posterior linear function')) + 
  theme_bw() + theme(legend.position = "bottom") + 
  xlab("x = displacement") + 
  ylab("y = mpg") + 
  scale_color_manual(name = '', values = c('Test data'='black',
                                           'Posterior linear function'= 'blue')) +
  ggtitle("Bayesian Linear Regression", 
          subtitle = "The estimated parameters are used in the linear model to make predictions using inputs from test data.\n RMSE = 6.611 and MAE = 4.954") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 20)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10))
p1
```

# Nonparametric Bayesian Methods  {data-background=#74d1ea}

## Challenges of Parametric Regression 

1. How do we pick the form of functions to use as our model?

```{r echo = FALSE, out.width = "80%"}
knitr::include_graphics("images/4plots.png")
```

2. How do we represent the uncertainty in our model due to?
      + Noisy data,
      + Poor model, etc.

## Intro to Bayesian Nonparametrics (1/4)

* Recall Bayes theorem for the parametric case:
      + $\theta$ are the unknown parameters we seek to learn e.g $\{\beta, \sigma \}$  
      
$$ P(\theta \mid y,x) = \frac{P(y \mid \theta,x) \, P(\theta)}{P(y \mid x)} $$

* In the **Nonparametric case**:
      + Instead, We want to learn the function $f$ that explains the data:
      
      $$y=f(x)+\varepsilon$$
      
      + $f$ is now treated as the unknown parameter we seek to learn  
<br>      
* Now modify the above Bayes theorem to non-parametric:
      + By placing prior over an $\infty$-dimensional space of functions, $p(f)$

$$ P(f\mid y,x) = \frac{P(y \mid f) \, P(f \mid x)}{P(y \mid x)} $$

## Intro to Bayesian Nonparametrics (2/4)

* What is this prior distribution over functions, $p(f)$?
      + It's the infinite set of imaginable functions
      + It's called a Gaussian Process (GP) prior  
      
```{r echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/gpsamples.png")
```

## Intro to Bayesian Nonparametrics (3/4)

* a GP prior is parameterized by: 
$$f(x) \sim GP(m(x), k(x, x'))$$
      + $m(x)$ = mean function
      + $k(x, x')$ = covariance (kernel) function  
<br>

* Any finite set of function evaluations $f(x_i)_1^N$ have a joint Gaussian distribution (Williams and Rasmussen 2006). 

```{r echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/gp-formula.png")
```

## Intro to Bayesian Nonparametrics (4/4)

* The updated Bayes formula for the GP posterior:
$$ P(f\mid y,x) = \frac{P(y \mid f) \, P(f \mid x)}{P(y \mid x)} = \frac{P(y \mid f) \, \mathcal{N}(m(x), K(x))}{P(y \mid x)} $$
<br>

* A fitted GP posterior on the same data looks like:
```{r echo = FALSE, out.width = "70%"}
knitr::include_graphics("images/1plotgp.png")
```
      + Shows both mean & covariance functions
      + Areas without data have high uncertainity & vice versa

## Bayesian <span style="color:green">Nonparametric</span> Model (1/5)
* The standard kernel is the ***Squared Exponential (SE)*** kernel, $k_y$, below:
$$k_y(x_i, x_j\mid\theta) = \sigma^2_fexp\bigg(-\frac{1}{2l^2}(x_i-x_j)^2\bigg) + \delta_{ij}\sigma^2_n$$
     + $\sigma^2_f$ = signal-variance parameter 
     + $l$ = length-scale parameter ($l$ is positive; controls smoothness) 
     + $\delta_{ij}$ = Kronecker delta function ($\delta_{ij}$ = 1 iff $i = j$ and 0 otherwise)
     + $\sigma^2_n$ = noise-variance parameter

## Bayesian <span style="color:green">Nonparametric</span> Model (2/5)
* **Step 1**: Define a prior, $p(f)$ on the parameter, $f$:
      + signal-variance: $\sigma_f \sim \mathcal{N}(0, 1)$
      + length-scale: $l \sim \mathcal{Gamma}(150, 1)$
      + noise-variance: $\sigma_n \sim \mathcal{N}(1, 0.1)$
      + prior-functions: $\textbf{y} \sim \mathcal{MultiNormal}(0, k_{\sigma^2_f,l}(x_i, x_j)+\sigma^2_n\textit{I}_n)$

```{r fig.height = 4, fig.align='default'}
length_scale_df <- data_frame(rgamma(n = 10000, shape = 150, rate = 1)) 
colnames(length_scale_df) <- "length_scale"
noise_variance_df <- data_frame(rnorm(n = 10000, mean = 1, sd = 0.1)) 
colnames(noise_variance_df) <- "noise_variance"
signal_variance_df <- data_frame(rnorm(n = 10000, mean = 0, sd = 1)) 
colnames(signal_variance_df) <- "signal_variance"
q2 <- ggplot() +
  geom_density(data = length_scale_df, aes(x = length_scale), fill = "pink", alpha = 0.2) +
  theme_bw() 
q3 <- ggplot() +
  geom_density(data = noise_variance_df, aes(x = noise_variance), fill = "red", alpha = 0.2) +
  theme_bw() 
q1 <- ggplot() +
  geom_density(data = signal_variance_df, aes(x = signal_variance), fill = "purple", alpha = 0.2) +
  theme_bw() 

gridExtra::grid.arrange(q1, q2, q3, ncol = 3)
```


## Bayesian <span style="color:green">Nonparametric</span> Model (3/5)
```{r echo = FALSE}
fit_sim_prior <- readRDS("fit_sim_prior.RDS")
list_of_draws_prior <- rstan::extract(fit_sim_prior, permuted=TRUE)
post_mu_fs_prior <- data.frame(x = seq(from = 50, to = 500, len = 2000), y = t(list_of_draws_prior$y), f = t(list_of_draws_prior$f))
datatable(post_mu_fs_prior[, -c(2:5)], class = 'cell-border stripe', 
          options = list(pageLength = 4))
```

<div>
```{r fig.height = 3, fig.width=10, fig.align = 'default'}
p2 <-  ggplot() + 
      theme_bw() +
      #geom_rect(xmin = -Inf, xmax = Inf, ymin = -2.0, ymax = 2.0, fill = "grey80") + 
      geom_line(data = post_mu_fs_prior, aes(x = x, y = f.1, color = "f1 prior function")) +
      geom_line(data = post_mu_fs_prior, aes(x = x, y = f.2, color = "f2 prior function")) +
      geom_line(data = post_mu_fs_prior, aes(x = x, y = f.3, color = "f3 prior function")) +
      geom_line(data = post_mu_fs_prior, aes(x = x, y = f.4, color = "f4 prior function")) +
      # geom_line(data = df_prior, aes(x = x, y = f_mean, color = "f function values")) +
      # geom_point(data = (data = dplyr::filter(post_mu_fs_melt, variable == "y.1"))[set,], 
      #            aes(x = x, y = value, colour = 'Sample observed data')) + 
      theme_bw() + theme(legend.position="bottom") + 
      scale_color_manual(name = '', values = c('Sample observed data'='black',
                                               'f1 prior function'='red',
                                               'f2 prior function'='blue',
                                               'f3 prior function'='green',
                                               'f4 prior function'='purple')) +
      xlab("x = displacement") + ylab("y = mpg") + 
      ggtitle("Four Samples from the Gaussian Process prior", 
              subtitle = "length-scale = 200, alpha = 1, sigma = 0.32")
ggplotly(p2)
```
</div>

## Bayesian <span style="color:green">Nonparametric</span> Model (4/5)
* **Step 2**: Define the likelihood
$$
\begin{aligned}
P(y \mid f, \sigma_n) = \mathcal{N}(y \mid f, \sigma^{2}_n) 
\end{aligned}
$$

* **Step 3**: Find the posterior of the function:
$$ 
\begin{aligned}
P(f\mid y,x) & \propto P(y \mid f, \sigma_n) \, P(f \mid x) \\
& \propto \mathcal{N}(y \mid f, \sigma^{2}_n) \, \mathcal{N}(m(x), k_y(x_i, x_j\mid\theta))
\end{aligned}
$$
<br>
```{r fig.height = 4}
library("bayesplot")
color_scheme_set("red")
fit_gp_current <- readRDS("fit_gp_current.RDS")
names(fit_gp_current)[1] <- "length_scale"
names(fit_gp_current)[2] <- "signal_variance"
names(fit_gp_current)[3] <- "noise_variance"
array_of_draws2 <- as.array(fit_gp_current)
mcmc_dens(array_of_draws2, 
          pars = c("length_scale", "signal_variance", "noise_variance"), 
          facet_args = list(labeller = label_parsed)) + 
          facet_text(size = 13, color = "darkblue") +
  ggtitle("Posterior distribution of Hyperparameters", 
          subtitle = "from Bayesian Nonparametric Regression") 
```

## Bayesian <span style="color:green">Nonparametric</span> Model (5/5)
* **Step 4**: Validate the model on test data
$$ 
P(\textbf{y}_*\mid\textbf{x}_*, \textbf{x, y}) = \int P(\textbf{f} \mid y,x) \, P(\textbf{y}_*\mid\textbf{x}_*, \textbf{f}) \, d\textbf{f}
$$

```{r out.width = "80%"}
knitr::include_graphics("images/posterior_gp_fit_mtcars.png")
```

# Appendix {data-background=#74d1ea}

## References

1. D. Emaasit, A. Paz and J. Salzwedel (2016).“A  Model-Based  Machine  Learning  Approach  for Capturing  Activity-Based  Mobility  Patterns  using  Cellular  Data”.

2. McKinsey & Company, (2013). http://bit.ly/2cJ1htj 

3. A. Campbell (2016). "Using Big Data to Measure Small Populations". Presented at the 2016 Conference on Innovations in Travel Modeling. Denver, Colorado, May 1st - 4th. 

4. Bishop, C. M. (2013). [Model-Based Machine Learning](http://rsta.royalsocietypublishing.org/content/371/1984/20120222). Philosophical Transactions of the Royal Society A, 371, pp 1–17

5. Winn, J., Bishop, C. M., Diethe, T. (2015). [Model-Based Machine Learning](http://www.mbmlbook.com). Microsoft Research Cambridge. http://www.mbmlbook.com.
